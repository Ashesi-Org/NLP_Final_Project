{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeBqKy3Lxnoj",
        "outputId": "07098a71-897e-4a48-a41a-0cb38cd7a8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.2.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#install all the necessary classes \n",
        "!pip install pandas numpy nltk scikit-learn gensim keras\n",
        "!pip install --upgrade keras tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gensim.downloader as api\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hO2OeZZ-HJbs"
      },
      "outputs": [],
      "source": [
        "# Load the dataset for Igbo, Hausa, and Nigerian Pidgin English\n",
        "# Read data from URLs\n",
        "igbo_df = pd.read_csv('https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/ibo/train.tsv', sep='\\t', header=None)\n",
        "hausa_df = pd.read_csv('https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/hau/train.tsv', sep='\\t', header=None)\n",
        "pidgin_df = pd.read_csv('https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/data/pcm/train.tsv', sep='\\t', header=None)\n",
        "\n",
        "# Rename columns\n",
        "igbo_df.columns = ['id', 'text']\n",
        "hausa_df.columns = ['id', 'text']\n",
        "pidgin_df.columns = ['id', 'text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qSw1_P5qLF6m"
      },
      "outputs": [],
      "source": [
        "# Merge the dataframes and shuffle the rows\n",
        "df = pd.concat([igbo_df, hausa_df, pidgin_df], ignore_index=True)\n",
        "df = df.sample(frac=1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zfK8xuRJLHtK"
      },
      "outputs": [],
      "source": [
        "# Define a function for preprocessing the text data\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # Remove stopwords\n",
        "    text_tokens = nltk.word_tokenize(text)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "    # Join the tokens back into a string\n",
        "    preprocessed_text = (\" \").join(tokens_without_sw)\n",
        "    return preprocessed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zDGvljdLLMio"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text data in the dataframe\n",
        "df['text'] = df['text'].apply(lambda x: preprocess_text(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SvwXok5RM_DM"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data and convert each word to a numerical representation\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['text'].values)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "X = tokenizer.texts_to_sequences(df['text'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5cxRfvF_NQob"
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to ensure they have the same length\n",
        "maxlen = 100\n",
        "X = pad_sequences(X, padding='post', maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels = df['id'].values\n",
        "encoder = LabelEncoder()\n",
        "Y = encoder.fit_transform(labels)\n",
        "Y = Y.reshape(-1,1)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2PB6TAsoOpjN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the deep learning model\n",
        "embedding_dim = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "23JStEA9Oqmm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "GYDU4zPxbAKS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q8VK5pOOxOX",
        "outputId": "9c890cbb-f0ac-4463-c6cd-e94c633915a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "590/590 [==============================] - 213s 355ms/step - loss: -733412.3750 - accuracy: 5.2989e-05 - val_loss: -1292735.6250 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "590/590 [==============================] - 212s 359ms/step - loss: -1845402.3750 - accuracy: 5.2989e-05 - val_loss: -2375561.2500 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "590/590 [==============================] - 212s 359ms/step - loss: -2936991.5000 - accuracy: 5.2989e-05 - val_loss: -3467138.7500 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "590/590 [==============================] - 220s 373ms/step - loss: -4033895.7500 - accuracy: 5.2989e-05 - val_loss: -4552679.5000 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "590/590 [==============================] - 270s 458ms/step - loss: -5124150.5000 - accuracy: 5.2989e-05 - val_loss: -5634964.5000 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "590/590 [==============================] - 251s 426ms/step - loss: -6211801.5000 - accuracy: 5.2989e-05 - val_loss: -6715652.0000 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "590/590 [==============================] - 230s 389ms/step - loss: -7298600.5000 - accuracy: 5.2989e-05 - val_loss: -7794321.0000 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "590/590 [==============================] - 243s 412ms/step - loss: -8385190.0000 - accuracy: 5.2989e-05 - val_loss: -8874988.0000 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "590/590 [==============================] - 206s 350ms/step - loss: -9472359.0000 - accuracy: 5.2989e-05 - val_loss: -9955369.0000 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "590/590 [==============================] - 214s 364ms/step - loss: -10559185.0000 - accuracy: 5.2989e-05 - val_loss: -11035069.0000 - val_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the testing set\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW-BHb71O-jA",
        "outputId": "506050f1-92f5-4233-bb2b-1830b67d05b8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0\n",
            "Test accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the evaluation results\n",
        "if score[1] >= 0.90:\n",
        "    print(\"The model performance is satisfactory and can be deployed for making predictions on new data.\")\n",
        "else:\n",
        "    print(\"The model performance is not satisfactory and needs to be further optimized.\")\n",
        "    # Perform model optimization\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_data=(X, Y))\n",
        "    # Evaluate the optimized model on the testing set\n",
        "    score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    # Print the evaluation results of the optimized model\n",
        "    print(\"Optimized model test loss:\", score[0])\n",
        "    print(\"Optimized model test accuracy:\", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4brMYLdbkF67",
        "outputId": "794a2631-cded-48ec-8aaa-f4c980877c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model performance is not satisfactory and needs to be further optimized.\n",
            "Epoch 1/50\n",
            "738/738 [==============================] - 380s 419ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 2/50\n",
            "738/738 [==============================] - 296s 401ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 3/50\n",
            "738/738 [==============================] - 335s 454ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 4/50\n",
            "738/738 [==============================] - 337s 457ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 5/50\n",
            "738/738 [==============================] - 341s 462ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 6/50\n",
            "738/738 [==============================] - 298s 403ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 7/50\n",
            "738/738 [==============================] - 352s 477ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 8/50\n",
            "738/738 [==============================] - 341s 462ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 9/50\n",
            "738/738 [==============================] - 333s 451ms/step - loss: 0.0000e+00 - accuracy: 4.2391e-05 - val_loss: 0.0000e+00 - val_accuracy: 3.3912e-05\n",
            "Epoch 10/50\n",
            "615/738 [========================>.....] - ETA: 41s - loss: 0.0000e+00 - accuracy: 5.0813e-05"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred).astype(int)"
      ],
      "metadata": {
        "id": "FfqClQXCUo_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the numerical predictions back to the original labels\n",
        "y_pred = encoder.inverse_transform(y_pred.reshape(-1,))\n",
        "Y_test = encoder.inverse_transform(Y_test.reshape(-1,))"
      ],
      "metadata": {
        "id": "jxmwCM8Je6w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print the classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "-CmrXkNKfG2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model\n",
        "model.save('language_classification_model.h5')"
      ],
      "metadata": {
        "id": "y3V0DBbRfMSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy and loss\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)"
      ],
      "metadata": {
        "id": "vq-uwpJHfTYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot training and validation accuracy\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p1Yzo5nRfq83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot training and validation loss\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "28dqav8PfyNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the trained model to make predictions on new text data\n",
        "#Load the saved model\n",
        "from keras.models import load_model\n",
        "model = load_model('language_classification_model.h5')"
      ],
      "metadata": {
        "id": "JoWgFV2Rf0IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_new_text(text):\n",
        "  # Convert text to lowercase\n",
        "  text = text.lower()\n",
        "  # Remove special characters\n",
        "  text = re.sub('.∗?.∗?', '', text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "  # Remove stopwords\n",
        "  text_tokens = nltk.word_tokenize(text)\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  # Join the tokens back into a string\n",
        "  preprocessed_text = (\" \").join(tokens_without_sw)\n",
        "  return preprocessed_text"
      ],
      "metadata": {
        "id": "bFkgYiLAf-K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function to classify new text data\n",
        "def classify_text(text):\n",
        "  preprocessed_text = preprocess_new_text(text)\n",
        "  X_new = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "  X_new = pad_sequences(X_new, padding='post', maxlen=maxlen)\n",
        "  y_new = model.predict(X_new)\n",
        "  y_new = np.round(y_new).astype(int)\n",
        "  language = encoder.inverse_transform(y_new.reshape(-1,))\n",
        "  return language[0]"
      ],
      "metadata": {
        "id": "9IQIFWclgH0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example usage of the classify_text function\n",
        "text1 = \"Ndi be anyi, unu a di mma?\"\n",
        "text2 = \"Mallam yaya ya juya masa karatu a hannunsa.\"\n",
        "text3 = \"I go chop am if dem give me food for this party.\"\n",
        "print(classify_text(text1))\n",
        "print(classify_text(text2))\n",
        "print(classify_text(text3))"
      ],
      "metadata": {
        "id": "q7dRP4w0hy1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model on the test set\n",
        "score, acc = model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
        "print(\"Test accuracy:\", acc)"
      ],
      "metadata": {
        "id": "PAj-qeTuh6tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred = np.round(Y_pred).astype(int)"
      ],
      "metadata": {
        "id": "BZPDYn88h-Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the predicted labels back to the original classes\n",
        "Y_pred = encoder.inverse_transform(Y_pred.ravel())\n",
        "Y_test = encoder.inverse_transform(Y_test.ravel())"
      ],
      "metadata": {
        "id": "cBYJmpWxiC6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "id": "dlQ3HfGSiI0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the trained model\n",
        "model.save('afrisenti_sentiment_model.h5')"
      ],
      "metadata": {
        "id": "mbSp3zTjiPQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}